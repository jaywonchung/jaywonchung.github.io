---
title: "[Review] XNOR-Nets: ImageNet Classification Using Binary Convolutional Neural Networks"
categories:
  - read
  - papers
---

# Resources
- [arXiv](https://arxiv.org/abs/1603.05279)
- [Official XNOR implementation of AlexNet](http://allenai.org/plato/xnornet)

# Abstract/Introduction
The two models presented:
> In Binary-Weight-Networks, the (convolution) filters are approximated with binary values resulting in 32 x memory saving.

> In XNOR-Networks, both the filters and the input to convolutional layers are binary. ... This results in 58 x faster convolutional operations...

Implications:
> XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time.

# Binary Convolutional Neural Networks
For future discussions we use the following mathematical notation for a CNN layer:  

$$ \mathcal{I}_{l(l=1,...,L)} = I\in \mathbb{R} ^{c \times w_{\text{in}} \times h_{\text{in}}}\text{ : input tensor for the }l^{\text{th}}\text{ layer} $$  
$$ \mathcal{W}_{lk(k=1,...,K^l)}=W \in \mathbb{R} ^{c \times w \times h}\text{ : }k^{\text{th}}\text{ weight filter for the }l^{\text{th}}\text{ layer} $$  
$$ \ast\text{ : convolution} $$  
$$ \oplus\text{ : convolution without multiplication} $$  
$$ \otimes \text{ : convolution with XNOR and bitcount} $$  
$$ \odot \text{ : elementwise multiplication} $$  

## Binary Convolution
In binary convolutional networks, we estimate the convolution filter weight as $$ W \approx \alpha B $$, where $$ \alpha $$ is a scalar scaling factor and $$ B \in \{+1, -1\} ^{c \times w \times h} $$. Hence, we estimate the convolution operation as follows:  

$$ I \ast W \approx (I \oplus B)\alpha $$  

To find an optimal estimation for $$ W\approx\alpha B $$ we solve the following problem:  

$$ J(B,\alpha)=\Vert W-\alpha B\Vert^2 $$  

$$ \alpha ^*,B^* =\underset{\alpha, B}{\text{argmin}}J(B,\alpha) $$  

Going straight to the answer:  

$$ \alpha^* = \frac{1}{n}\Vert W \Vert_{l1} $$  

$$ B^*=\text{sign}(W) $$  


## Training
The gradients are computed as follows:  

$$ \frac{\partial \text{sign}}{\partial r}=r \text{1}_{\vert r \vert \le1} $$  


# Discussion

Back Prop을 할 때는 full precision을 사용해야 한 다는 점이 유일하게 아쉽다. 온전히 binary로만 이루어진 건 아님.
